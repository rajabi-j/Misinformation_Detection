{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J144yFUkeZQQ"
      },
      "source": [
        "## Show The Events with highest Vocabulary Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p4TfwoNppgaX"
      },
      "outputs": [],
      "source": [
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "65gkV-8MptFO"
      },
      "outputs": [],
      "source": [
        "pkl_file_path = 'train_w_time_10232023.pkl'\n",
        "\n",
        "with open(pkl_file_path, 'rb') as file:\n",
        "    train_data = pd.read_pickle(file)\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "\n",
        "pkl_file_path = 'test_w_time_10232023.pkl'\n",
        "\n",
        "with open(pkl_file_path, 'rb') as file:\n",
        "    test_data = pd.read_pickle(file)\n",
        "\n",
        "test_df = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb5_Aoz0p1ZO",
        "outputId": "e41ab559-b7d1-4dfe-ae34-60538f159605"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['tweet_id', 'lang', 'res_image_vectors', 'vgg_image_vectors',\n",
              "       'tweet_text', 'label', 'event', 'new_img_id', 'clean_tweet', 'cl_len',\n",
              "       'text_feats_1', 'text_feats_2', 'pooler', 'sm_last_four_concat',\n",
              "       'mn_last_four_concat', 'mn_2ndtolast_lyr', 'sm_2ndtolast_lyr',\n",
              "       'num_friends', 'num_followers', 'folfriend_ratio', 'times_listed',\n",
              "       'has_url', 'is_verified', 'num_posts', 'num_words', 'text_length',\n",
              "       'contains_questmark', 'num_questmark', 'contains_exclammark',\n",
              "       'num_exclammark', 'contains_happyemo', 'contains_sademo',\n",
              "       'contains_firstorderpron', 'contains_secondorderpron',\n",
              "       'contains_thirdorderpron', 'num_uppercasechars', 'num_possentiwords',\n",
              "       'num_negsentiwords', 'num_mentions', 'num_hashtags', 'num_URLs',\n",
              "       'num_retweets', 'semi_clean', 'reading_ease', 'reading_grade',\n",
              "       'compound_score', 'neg_score', 'pos_score', 'neutral_score',\n",
              "       'vgg19_img_vectors', 'Topic20', 'Topic10', 'Topic5', 'caption',\n",
              "       'capt_feats', 'Topic200', 'Topic300', 'new_ev', 'event_feat',\n",
              "       'Topic200_200', 'Topic200_300', 'Topic200_500', 'timestamp', 'weekday',\n",
              "       'mdy', 'tm', 'mdy_tm'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming context_df and context_df_test are already defined Pandas DataFrames\n",
        "concatenated_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "concatenated_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXWur-eip4Jn",
        "outputId": "1055a90f-1b73-467e-ba01-afc586335d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tweet_text     Don't need feds to solve the #bostonbombing when we have #4chan!! http://t.co/eXQTPZqqbG\n",
            "clean_tweet                                              [don, need, feds, solve, bostonbombing, 4chan]\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "with pd.option_context('display.max_colwidth', None):\n",
        "  selected_columns = ['tweet_text', 'clean_tweet']\n",
        "  print(concatenated_df[selected_columns].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNzna_sGp6i1",
        "outputId": "0645c9af-7b30-492b-934d-0f9349dee5d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "386"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(concatenated_df['mdy'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUGfXtY4p8lG",
        "outputId": "3f8851ff-63d1-4ed0-ff5c-c6a912c20410"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9938, 67)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBRDAOBxqBmB",
        "outputId": "ee84e3d5-6252-4361-bf99-14055e0a64ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['boston', 'malaysia', 'passport', 'sandy', 'sochi', 'bringback',\n",
              "       'columbianChemicals', 'elephant', 'livr', 'pigFish', 'underwater',\n",
              "       'eclipse', 'samurai', 'nepal', 'garissa', 'fuji_lenticular',\n",
              "       'bush_book', 'gandhi_dancing', 'attacks_paris', 'bowie_david',\n",
              "       'pakistan_explosion', 'refugees', 'protest', 'not_afraid',\n",
              "       'rio_moon', 'immigrants', 'john_guevara', 'syrian_children',\n",
              "       'mc_donalds', 'brussels_explosions', 'half_everything',\n",
              "       'hubble_telescope', 'woman_14', 'north_korea', 'five_headed',\n",
              "       'burst_kfc', 'black_lion', 'nazi_submarine'], dtype=object)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "concatenated_df['event'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdMa5ZL3qEbx",
        "outputId": "0ba52d35-b7a7-4a2d-a4b5-f32624e34a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2324\n",
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9938"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_text = concatenated_df.groupby('mdy')\n",
        "time_counts = time_text['tweet_text'].count()\n",
        "print(time_counts.max())\n",
        "print(time_counts.min())\n",
        "time_counts.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2eazi13qPit",
        "outputId": "84ce5638-58b4-438c-8edc-2c88d00a1720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "event\n",
              "attacks_paris           181\n",
              "black_lion                7\n",
              "boston                  474\n",
              "bowie_david              64\n",
              "bringback               109\n",
              "brussels_explosions       9\n",
              "burst_kfc                15\n",
              "bush_book                25\n",
              "columbianChemicals      182\n",
              "eclipse                 200\n",
              "elephant                 11\n",
              "five_headed               5\n",
              "fuji_lenticular         147\n",
              "gandhi_dancing           13\n",
              "garissa                  44\n",
              "half_everything          34\n",
              "hubble_telescope         17\n",
              "immigrants               30\n",
              "john_guevara             10\n",
              "livr                      2\n",
              "malaysia                165\n",
              "mc_donalds                5\n",
              "nazi_submarine           10\n",
              "nepal                  1250\n",
              "north_korea               7\n",
              "not_afraid               48\n",
              "pakistan_explosion       34\n",
              "passport                 41\n",
              "pigFish                  12\n",
              "protest                  34\n",
              "refugees                 53\n",
              "rio_moon                 26\n",
              "samurai                 213\n",
              "sandy                  6073\n",
              "sochi                   259\n",
              "syrian_children          11\n",
              "underwater              107\n",
              "woman_14                 11\n",
              "Name: event, dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "event_text = concatenated_df.groupby('event')\n",
        "event_counts = event_text['event'].count()\n",
        "event_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RBgIlm55qRrg"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "def convert_datetime(date_time):\n",
        "\n",
        "    formatted_date = date_time.strftime('%y%m%d')\n",
        "\n",
        "    return formatted_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 33, Event: sandy, Tweet Count: 6073, Vocab Size: 4963\n",
            "Time: 23, Event: nepal, Tweet Count: 1250, Vocab Size: 1060\n",
            "Time: 2, Event: boston, Tweet Count: 474, Vocab Size: 962\n",
            "Time: 34, Event: sochi, Tweet Count: 259, Vocab Size: 800\n",
            "Time: 20, Event: malaysia, Tweet Count: 165, Vocab Size: 426\n",
            "Time: 8, Event: columbianChemicals, Tweet Count: 182, Vocab Size: 407\n",
            "Time: 9, Event: eclipse, Tweet Count: 200, Vocab Size: 325\n",
            "Time: 12, Event: fuji_lenticular, Tweet Count: 147, Vocab Size: 270\n",
            "Time: 0, Event: attacks_paris, Tweet Count: 181, Vocab Size: 254\n",
            "Time: 4, Event: bringback, Tweet Count: 109, Vocab Size: 233\n",
            "Time: 30, Event: refugees, Tweet Count: 53, Vocab Size: 217\n",
            "Time: 3, Event: bowie_david, Tweet Count: 64, Vocab Size: 184\n",
            "Time: 14, Event: garissa, Tweet Count: 44, Vocab Size: 177\n",
            "Time: 17, Event: immigrants, Tweet Count: 30, Vocab Size: 167\n",
            "Time: 25, Event: not_afraid, Tweet Count: 48, Vocab Size: 147\n",
            "Time: 29, Event: protest, Tweet Count: 34, Vocab Size: 145\n",
            "Time: 7, Event: bush_book, Tweet Count: 25, Vocab Size: 130\n",
            "Time: 32, Event: samurai, Tweet Count: 213, Vocab Size: 108\n",
            "Time: 36, Event: underwater, Tweet Count: 107, Vocab Size: 106\n"
          ]
        }
      ],
      "source": [
        "# List to hold integrated data\n",
        "events_data = []\n",
        "all_unique_elements = set()\n",
        "\n",
        "# Iterate through the grouped data\n",
        "time_t = 0\n",
        "for event_n, group in event_text:\n",
        "    all_elements = []\n",
        "    unique_elements = set()\n",
        "    for elements in group['clean_tweet']:\n",
        "        all_elements.extend(elements)\n",
        "        unique_elements.update(elements)\n",
        "\n",
        "    # Create a dictionary for the current time point and add it to the list\n",
        "    events_data.append({\n",
        "        'time_t': time_t,\n",
        "        'text': all_elements,\n",
        "        'vocab': list(unique_elements),\n",
        "        'event': event_n,\n",
        "        'tweet_count': len(group)\n",
        "    })\n",
        "\n",
        "    # Update all unique elements\n",
        "    all_unique_elements.update(unique_elements)\n",
        "\n",
        "    # Increment time index\n",
        "    time_t += 1\n",
        "\n",
        "# Update total vocabulary\n",
        "total_vocab = list(all_unique_elements)\n",
        "\n",
        "# Sort integrated_data based on the length of 'vocab' in descending order\n",
        "events_data.sort(key=lambda x: len(x['vocab']), reverse=True)\n",
        "\n",
        "# Now integrated_data is sorted based on vocab size descending\n",
        "for data in events_data[:19]:\n",
        "    print(f\"Time: {data['time_t']}, Event: {data['event']}, Tweet Count: {data['tweet_count']}, Vocab Size: {len(data['vocab'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7791"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(total_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event</th>\n",
              "      <th>tweet_count</th>\n",
              "      <th>vocab_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sandy</td>\n",
              "      <td>6073</td>\n",
              "      <td>4963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nepal</td>\n",
              "      <td>1250</td>\n",
              "      <td>1060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boston</td>\n",
              "      <td>474</td>\n",
              "      <td>962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sochi</td>\n",
              "      <td>259</td>\n",
              "      <td>800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>malaysia</td>\n",
              "      <td>165</td>\n",
              "      <td>426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>columbianChemicals</td>\n",
              "      <td>182</td>\n",
              "      <td>407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>eclipse</td>\n",
              "      <td>200</td>\n",
              "      <td>325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fuji_lenticular</td>\n",
              "      <td>147</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>attacks_paris</td>\n",
              "      <td>181</td>\n",
              "      <td>254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>bringback</td>\n",
              "      <td>109</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>refugees</td>\n",
              "      <td>53</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>bowie_david</td>\n",
              "      <td>64</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>garissa</td>\n",
              "      <td>44</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>immigrants</td>\n",
              "      <td>30</td>\n",
              "      <td>167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>not_afraid</td>\n",
              "      <td>48</td>\n",
              "      <td>147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>protest</td>\n",
              "      <td>34</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bush_book</td>\n",
              "      <td>25</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>samurai</td>\n",
              "      <td>213</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>underwater</td>\n",
              "      <td>107</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 event  tweet_count  vocab_size\n",
              "1                sandy         6073        4963\n",
              "2                nepal         1250        1060\n",
              "3               boston          474         962\n",
              "4                sochi          259         800\n",
              "5             malaysia          165         426\n",
              "6   columbianChemicals          182         407\n",
              "7              eclipse          200         325\n",
              "8      fuji_lenticular          147         270\n",
              "9        attacks_paris          181         254\n",
              "10           bringback          109         233\n",
              "11            refugees           53         217\n",
              "12         bowie_david           64         184\n",
              "13             garissa           44         177\n",
              "14          immigrants           30         167\n",
              "15          not_afraid           48         147\n",
              "16             protest           34         145\n",
              "17           bush_book           25         130\n",
              "18             samurai          213         108\n",
              "19          underwater          107         106"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming integrated_data is already filled as per the previous discussion\n",
        "df = pd.DataFrame(events_data)\n",
        "\n",
        "# Adding a new column to count vocab size\n",
        "df['vocab_size'] = df['vocab'].apply(len)\n",
        "\n",
        "# Display the top 19 entries\n",
        "top_19 = df[[ 'event', 'tweet_count', 'vocab_size']].head(19)\n",
        "\n",
        "# Resetting the index to start from 1\n",
        "top_19.index = range(1, len(top_19) + 1)\n",
        "\n",
        "top_19"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
